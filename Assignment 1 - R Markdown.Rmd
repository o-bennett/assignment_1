---
title: "Assignment 1"
author: "Oliver Bennett (2014247)"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r source, echo = FALSE, message = FALSE, results = 'hide', warning = FALSE, cache = TRUE}
source("C:R/EC349 Assignment 1 - 2014247.R")
```
#### Word Count (excluding tables): 1210

## Tabula statement

We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

Privacy statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

Related articles

[Reg. 11 Academic Integrity (from 4 Oct 2021)](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwarwick.ac.uk%2Fservices%2Fgov%2Fcalendar%2Fsection2%2Fregulations%2Facademic_integrity%2F&data=05%7C01%7COliver.Bennett.1%40warwick.ac.uk%7C4dd142f8cc154001823908dbf19221e9%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638369382184961738%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=u661a0PI1LD1FJkpCa3TbxyDKOSilXIgywmEq7nRrFA%3D&reserved=0)

[Guidance on Regulation 11](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwarwick.ac.uk%2Fservices%2Faro%2Fdar%2Fquality%2Faz%2Facintegrity%2Fframework%2Fguidancereg11%2F&data=05%7C01%7COliver.Bennett.1%40warwick.ac.uk%7C4dd142f8cc154001823908dbf19221e9%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638369382184970867%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=fyIOHxNjiYyCT3SLjH1ZCPR5O4cIpuZ35qVno%2BBA25Q%3D&reserved=0)

[Proofreading Policy](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwarwick.ac.uk%2Fservices%2Faro%2Fdar%2Fquality%2Fcategories%2Fexaminations%2Fpolicies%2Fv_proofreading%2F&data=05%7C01%7COliver.Bennett.1%40warwick.ac.uk%7C4dd142f8cc154001823908dbf19221e9%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638369382184977389%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=zhp1VEUsWofNTxkouyprL1%2FHSD5guqYjMxo4vvMtQsA%3D&reserved=0)

[Education Policy and Quality Team](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwarwick.ac.uk%2Fservices%2Faro%2Fdar%2Fquality%2Faz%2Facintegrity%2Fframework%2Fguidancereg11%2F&data=05%7C01%7COliver.Bennett.1%40warwick.ac.uk%7C4dd142f8cc154001823908dbf19221e9%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638369382184983755%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=dnqdD5algYtO8rCT5RKNSqqY8Thzs9crnW8lVdnq0vs%3D&reserved=0)

[Academic Integrity (warwick.ac.uk)](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwarwick.ac.uk%2Fstudents%2Flearning-experience%2Facademic_integrity&data=05%7C01%7COliver.Bennett.1%40warwick.ac.uk%7C4dd142f8cc154001823908dbf19221e9%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638369382184990089%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=P9qABE735BU0TQMBXqyP2jB3%2Bvqb0Rm9Uy9fdhPq5K0%3D&reserved=0)

This is the end of the statement to be included.

## Methodology

In this assignment, I chose to follow the Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology. CRISP-DM offers a structured approach, split into six stages: business understanding, data understanding, data preparation, modelling, evaluation, and deployment. I decided on CRISP-DM, due to its iterative nature and reputation as an industry-proven comprehensive approach to managing data science projects. The iterative nature was crucial in the development of my model as it allowed me to continually refine my process based on evolving insights. The structured approach was also key in the completion of this assignment. The business and data understanding stages were crucial in framing the problem and guiding my choices of variables and models. In the modelling phase I iteratively tested and refined these models, gaining insights that allowed me to better prepare the data, and the evaluation phase allowed me to compare the performance of the different models.

#### CRISP-DM

<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png" width="400" />
</p>

## Business Understanding

[yelp.com](yelp.com) is a website that publishes crowd-sourced reviews about businesses. This project aims to use information on users, reviews, and businesses on [yelp.com](yelp.com) to predict how users like different establishments. The goal of this project is to create a model that accurately predicts the number of stars that reviews gave, and I will follow the stages of the CRISP-DM methodology to achieve this.

## Data Understanding

A strong understanding of the available data is crucial in identifying which variables are relevant for prediction, assessing the quality of data, and guiding the selection of the most appropriate modelling techniques. 5 data sets were provided for this assignment: data on the businesses, user reviews, user “check-ins”, users and user tips. Using information available on the datasets from the [Yelp website]( https://www.yelp.com/dataset/documentation/main), I identified that only the datasets on businesses, user reviews, and users would be necessary for predicting reviews, and I merged these datasets into one dataframe `merged_data`. Using this dataframe I was able to perform visualisation techniques to view characteristics of the data set, such as the distribution of reviews.

#### Distribution of Reviews

```{r review histogram, echo=FALSE, cache = TRUE}
# Exploratory Data Analytics
# Histogram for distribution of review stars
hist(merged_data$stars.x, breaks = seq(0.5, 5.5, by = 1), main = "", xlab = "Stars", xaxt = "n")
axis(1, at = 1:5, labels = 1:5)
```

By further consulting the [Yelp website]( https://www.yelp.com/dataset/documentation/main) and analysing the `merged_data` dataframe, I was also able to gain a deeper understanding of the available variables.

#### Variables in `merged_data`
  
```{r , echo = FALSE, cache = TRUE}
glimpse(merged_data)
```

Based on the available data and the predictive goal of the assignment, I chose to create OLS, Ridge, LASSO, regression tree and classification tree models. The use of a classification tree model makes the most sense as the number of stars in a review is categorical - users can only give a whole number of stars from 1-5. However, the other models can also be used as the number of stars is a numerical variable.

## Data Preparation

When preparing the data for modelling, I removed several variables including string type variables such as `business.id`, `name.x` and `text`, as my models would only work if the variables were either numerical or factor type. I then removed variables such as `city` and `categories`, as while these could be converted to factor types, when I did so they had very high levels (1000+ and 48000+ respectively) and so would not be useful in predictive models. In addition, I removed variables which had a high percentage of *NA* observations (>50%) as these also would not be useful in prediction. 

I also modified several variables so that they could be used in the predictive models. This included Using the ‘lubridate’ package on the string type date variable to create factor variables for the year, month and day of the week that the reviews were written.

After these changes I was left with a `clean_data` dataframe that contained 30 unique variables, as seen below:

#### Variables Description

| Variable | Type (in `clean_data`) | Description |
|:----:|:----:|---------|
| `useful.x` | Numeric (integer) | Number of useful votes sent my the user |
| `funny.x` | Numeric (integer) | Number of funny votes sent my the user |
| `cool.x` | Numeric (integer) | Number of cool votes sent my the user |
| `review_count.x` | Numeric (integer) | Number of reviews the user has written |
| `useful.y` | Numeric (integer) | Number of useful votes the review received |
| `funny.y` | Numeric (integer) | Number of funny votes the review received |
| `cool.y` | Numeric (integer) | Number of cool votes the review received |
| `elite` | Numeric (integer) | Number of years the user was elite |
| `friends` | Numeric (integer) | Number of friends the user has |
| `fans` | Numeric (integer) | Number of fans the user has |
| `average_stars` | Numeric (float) | Average rating of all reviews written by the user |
| `compliment_hot` | Numeric (integer) | Number of hot compliments received by the user |
| `compliment_more` | Numeric (integer) | Number of more compliments received by the user |
| `compliment_profile` | Numeric (integer) | Number of profile compliments received by the user |
| `compliment_cute` | Numeric (integer) | Number of cute compliments received by the user |
| `compliment_list` | Numeric (integer) | Number of list compliments received by the user |
| `compliment_note` | Numeric (integer) | Number of note compliments received by the user |
| `compliment_plain` | Numeric (integer) | Number of plain compliments received by the user |
| `compliment_cool` | Numeric (integer) | Number of cool compliments received by the user |
| `compliment_writer` | Numeric (integer) | Number of writer compliments received by the user |
| `compliment_photos` | Numeric (integer) | Number of photo compliments received by the user |
| `state` | Factor(14 levels) | State that the business is located in |
| `stars.y` | Numeric (float) | Star rating of the business, rounded to half stars |
| `review_count.y`| Numeric (integer) | Number of reviews written about the business |
| `is_open` | Numeric (binary) | 0 if the business is closed, 1 if open |
| `account_age` | Numeric (integer) | Age of account (days) when the review was posted |
| `review_year` | Factor (18 levels) | Year  the review was posted |
| `review_month` | Factor (12 levels) | Month the review was posted |
| `review_day_of_week` | Factor (7 levels) | Day of the week the review was posted |
| `review_stars` | Numeric (integer) | Stars given in the review (the dependent variable) |

#### Summary of Variables

```{r , echo = FALSE, cache = TRUE}
summary(clean_data)
```

After selecting and modifying my variables I then created 3 data sets, containing identical observations, to be used in different models as detailed below:

| Dataset | Description |
|----|------------------|
| `numeric_df` | Matrix containing only numerical variables. To be used for Ridge and LASSO models |
| `clean_data` | Dataframe containing both numerical and factor type variables with the dependent variable `review_stars` as a numerical type. To be used for OLS and regression tree models |
| `categorical_data` | Same as `clean_data` but `review_stars` is converted to a factor type. To be used for randomly generated reviews and classification tree models |

I regularly revisited and refined the data preparation stage, making adjustments to the dataframe as I gained new insights in subsequent stages. This included removing the variable `compliment_funny`, which was generating *NA* coefficients, and removing some observations from states with few reviews.

#### Number of Reviews by State

```{r state counts, echo = FALSE, cache = TRUE}
library(knitr)
transposed_state_counts <- t(state_counts)
kable(transposed_state_counts)
```

## Modeling

In the modeling phase of the CRISP-DM process, I implemented both Ridge and LASSO regression techniques, as well as Ordinary Least Squares (OLS) to serve as a baseline for comparison. Ridge and LASSO were chosen due to their effectiveness in handling multicollinearity and their regularization capabilities which enhance model performance, especially in scenarios with numerous predictors like this. The inclusion of penalty terms in both Ridge and Lasso helps to reduce overfitting, a common challenge in predictive modeling.

$\hat{\beta}^{OLS} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \right\}$.

$\hat{\beta}^{Ridge} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}$.

$\hat{\beta}^{LASSO} = \arg\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}$.

For both the Ridge and Lasso regression models, standardization of predictor variables was a crucial step, as these techniques penalize larger coefficients more heavily.

To optimize the models and prevent overfitting, I utilized cross-validation. This technique is critical for assessing the effectiveness of the models and for selecting the optimal value of the lambda parameter that minimises the mean squared error, balancing the trade-off between bias and variance.

#### Ridge Lambda Selection

```{r , echo = FALSE, message = FALSE, results = 'hide', warning = FALSE, cache = TRUE}
cv.out_ridge <- cv.glmnet(as.matrix(ridgex_train), as.matrix(ridgey_train), alpha = 0, nfolds = 3)
plot(cv.out_ridge)
```

#### LASSO Lambda Selection

```{r , echo = FALSE, cache = TRUE}
cv.out_LASSO <- cv.glmnet(as.matrix(ridgex_train), as.matrix(ridgey_train), alpha = 1, nfolds = 3)
plot(cv.out_LASSO)
```

A key characteristic of Lasso is its ability to reduce some coefficients exactly to zero, effectively performing feature selection. We can see this in the graph above, as the number above the graph (indicating the number of variables used in the model) is decreasing as lambda increases in the LASSO model. Ridge regression tends to shrink the coefficients towards zero, but it rarely sets any coefficients exactly to zero and so this is why the numbers above the graph stay constant at 73.

In addition to Ridge and Lasso, I also employed regression and classification trees in my analysis. These trees provide a more intuitive understanding of how the input variables affect the predicted review scores and allow for a non-linear approach to modeling. 

#### Regression Tree

```{r , echo = FALSE, cache = TRUE}
rpart.plot(reg_tree)
```
 
#### Classification Tree 
 
```{r , echo = FALSE, cache = TRUE}
rpart.plot(class_tree)
```
 
Finally, to establish a benchmark for comparison of prediction accuracy, I generated a set of random reviews.

## Evaluation

MSE provides a clear numerical value that represents the average of the squares of the errors, essentially quantifying how far the model's predictions are from the actual values. 

$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

#### MSE Evaluation for Regressions
| Model | Training MSE | Test MSE |
|:--------:|:---------:|:---------:|
| <span style="color:blue">OLS</span> | <span style="color:blue">1.176837</span> | <span style="color:blue">1.183622</span> |
| Ridge | 1.180266 | 1.18888 |
| LASSO | 1.178036 | 1.183871 |
| Regression tree | 1.313129 | 1.313515 |


Prediction accuracy measures the percentage of predictions that the model gets correct.

#### Prediction Accuracy Evaluation for Classification Models
| Model | Training Accuracy | Test Accuracy |
|:--------:|:---------:|:---------:|
| Randomly generated | 0.2006618 | 0.203 |
| <span style="color:blue">Classification tree</span> | <span style="color:blue">0.5503822</span> | <span style="color:blue">0.5523</span> |
| Rounded Ridge | 0.3853973 | 0.3838 |
| Rounded Lasso | 0.3895882 | 0.3885 |

As expected, the classification tree emerged as the top performer among the classification models with a prediction accuracy of 55%. Surprisingly, the OLS model outperformed both Ridge and LASSO in terms of MSE, suggesting a possible lack of multicollinearity and overfitting in our data. This hypothesis is further supported by the near-zero lambda values used in Ridge and LASSO, indicating minimal impact of the regularization term. Additionally, the consistency between training and test MSEs across models corroborates the absence of overfitting, demonstrating effective generalization to new data.

OLS has the added benefit of having interpretable coefficients, which can be seen below:

```{r , echo = FALSE, cache = TRUE}
summary(lm_review)
```

The R-squared value is 0.4617, suggesting that about 46.17% of the variability in review_stars is explained by the model, which is a moderate level of explanatory power.The Adjusted R-squared is almost the same (0.4616), indicating that the number of predictors in the model is reasonable and not excessively inflating the R-squared value.

## Challenges

I became quite stuck at one point, as I was achieving extremely low MSEs for the Ridge and LASSO models, and I was unsure why. Upon reviewing the `numeric_df` dataset, I realized the predicted variable had been inadvertently standardized along with the predictors. Correcting this by removing the relevant line in my code resolved the issue.

